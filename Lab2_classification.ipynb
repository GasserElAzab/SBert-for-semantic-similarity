{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616cc080",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f8bc1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models,evaluation\n",
    "from transformers import BertTokenizer\n",
    "from csv import QUOTE_NONE\n",
    "from torch.utils.data import DataLoader,Dataset, TensorDataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5012a4",
   "metadata": {},
   "source": [
    "# Loading the data and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d3162ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinli_train = pd.read_json(\"multinli_1.0/multinli_1.0_train.jsonl\", lines=True)\n",
    "multinli_test = pd.read_json(\"multinli_1.0/multinli_1.0_dev_matched.jsonl\", lines=True)\n",
    "# multinli_mismatched = pd.read_json(\"multinli_1.0/multinli_1.0_dev_mismatched.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4467265a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>promptID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>31193n</td>\n",
       "      <td>31193</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>( ( Conceptually ( cream skimming ) ) ( ( has ...</td>\n",
       "      <td>(ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>( ( ( Product and ) geography ) ( ( are ( what...</td>\n",
       "      <td>(ROOT (S (NP (NN Product) (CC and) (NN geograp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>101457e</td>\n",
       "      <td>101457</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>( you ( ( know ( during ( ( ( the season ) and...</td>\n",
       "      <td>(ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>( You ( ( ( ( lose ( the things ) ) ( to ( the...</td>\n",
       "      <td>(ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>134793e</td>\n",
       "      <td>134793</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>( ( One ( of ( our number ) ) ) ( ( will ( ( (...</td>\n",
       "      <td>(ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>( ( ( A member ) ( of ( my team ) ) ) ( ( will...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>37397e</td>\n",
       "      <td>37397</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>( ( How ( ( ( do you ) know ) ? ) ) ( ( All th...</td>\n",
       "      <td>(ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>( ( This information ) ( ( belongs ( to them )...</td>\n",
       "      <td>(ROOT (S (NP (DT This) (NN information)) (VP (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>50563n</td>\n",
       "      <td>50563</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>( yeah ( i ( ( tell you ) ( what ( ( though ( ...</td>\n",
       "      <td>(ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB ...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>( ( The ( tennis shoes ) ) ( ( have ( ( a rang...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (NN tennis) (NNS shoes))...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotator_labels       genre  gold_label   pairID  promptID  \\\n",
       "0        [neutral]  government     neutral   31193n     31193   \n",
       "1     [entailment]   telephone  entailment  101457e    101457   \n",
       "2     [entailment]     fiction  entailment  134793e    134793   \n",
       "3     [entailment]     fiction  entailment   37397e     37397   \n",
       "4        [neutral]   telephone     neutral   50563n     50563   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0  Conceptually cream skimming has two basic dime...   \n",
       "1  you know during the season and i guess at at y...   \n",
       "2  One of our number will carry out your instruct...   \n",
       "3  How do you know? All this is their information...   \n",
       "4  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                              sentence1_binary_parse  \\\n",
       "0  ( ( Conceptually ( cream skimming ) ) ( ( has ...   \n",
       "1  ( you ( ( know ( during ( ( ( the season ) and...   \n",
       "2  ( ( One ( of ( our number ) ) ) ( ( will ( ( (...   \n",
       "3  ( ( How ( ( ( do you ) know ) ? ) ) ( ( All th...   \n",
       "4  ( yeah ( i ( ( tell you ) ( what ( ( though ( ...   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "0  (ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...   \n",
       "1  (ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...   \n",
       "2  (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...   \n",
       "3  (ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do...   \n",
       "4  (ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB ...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  Product and geography are what make cream skim...   \n",
       "1  You lose the things to the following level if ...   \n",
       "2  A member of my team will execute your orders w...   \n",
       "3                  This information belongs to them.   \n",
       "4           The tennis shoes have a range of prices.   \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "0  ( ( ( Product and ) geography ) ( ( are ( what...   \n",
       "1  ( You ( ( ( ( lose ( the things ) ) ( to ( the...   \n",
       "2  ( ( ( A member ) ( of ( my team ) ) ) ( ( will...   \n",
       "3  ( ( This information ) ( ( belongs ( to them )...   \n",
       "4  ( ( The ( tennis shoes ) ) ( ( have ( ( a rang...   \n",
       "\n",
       "                                     sentence2_parse  \n",
       "0  (ROOT (S (NP (NN Product) (CC and) (NN geograp...  \n",
       "1  (ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...  \n",
       "2  (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...  \n",
       "3  (ROOT (S (NP (DT This) (NN information)) (VP (...  \n",
       "4  (ROOT (S (NP (DT The) (NN tennis) (NNS shoes))...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinli_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7451a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entailment</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entailment</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entailment</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gold_label                                          sentence1  \\\n",
       "0     neutral  Conceptually cream skimming has two basic dime...   \n",
       "1  entailment  you know during the season and i guess at at y...   \n",
       "2  entailment  One of our number will carry out your instruct...   \n",
       "3  entailment  How do you know? All this is their information...   \n",
       "4     neutral  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                                           sentence2  \n",
       "0  Product and geography are what make cream skim...  \n",
       "1  You lose the things to the following level if ...  \n",
       "2  A member of my team will execute your orders w...  \n",
       "3                  This information belongs to them.  \n",
       "4           The tennis shoes have a range of prices.  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinli_train_reduced = pd.concat([multinli_train[i] for i in [\"gold_label\", \"sentence1\",\"sentence2\"]], axis=1)\n",
    "multinli_test_reduced = pd.concat([multinli_test[i] for i in [\"gold_label\", \"sentence1\",\"sentence2\"]], axis=1)\n",
    "multinli_train_reduced.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0409faaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'entailment' 'contradiction']\n",
      "['neutral' 'contradiction' 'entailment' '-']\n",
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(multinli_train_reduced['gold_label'].unique())\n",
    "print(multinli_test_reduced['gold_label'].unique())\n",
    "print(multinli_test_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd5c9577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9815, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinli_test_reduced= multinli_test_reduced.loc[multinli_test_reduced[\"gold_label\"] != '-']\n",
    "multinli_test_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2aab4962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>The new rights are nice enough</td>\n",
       "      <td>Everyone really likes the newest benefits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>This site includes a list of all award winners...</td>\n",
       "      <td>The Government Executive articles housed on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entailment</td>\n",
       "      <td>uh i don't know i i have mixed emotions about ...</td>\n",
       "      <td>I like him for the most part, but would still ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>yeah i i think my favorite restaurant is alway...</td>\n",
       "      <td>My favorite restaurants are always at least a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>i don't know um do you do a lot of camping</td>\n",
       "      <td>I know exactly.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gold_label                                          sentence1  \\\n",
       "0        neutral                     The new rights are nice enough   \n",
       "1  contradiction  This site includes a list of all award winners...   \n",
       "2     entailment  uh i don't know i i have mixed emotions about ...   \n",
       "3  contradiction  yeah i i think my favorite restaurant is alway...   \n",
       "4  contradiction         i don't know um do you do a lot of camping   \n",
       "\n",
       "                                           sentence2  \n",
       "0         Everyone really likes the newest benefits   \n",
       "1  The Government Executive articles housed on th...  \n",
       "2  I like him for the most part, but would still ...  \n",
       "3  My favorite restaurants are always at least a ...  \n",
       "4                                    I know exactly.  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinli_test_reduced.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c41b9",
   "metadata": {},
   "source": [
    "# Trying training the BERT in pytorch (time consuming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab695261",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This class is adapted from https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db\n",
    "This step is done to tokenize the dataset such as [CLS],[SEP],etc. In addition, marking the position of each sentence. (WE ARE INTERESTED TO GET THE SENTENCE EMBEDDING FIRST -->\n",
    "then)\n",
    "'''\n",
    "# class MNLIDataBert(Dataset):\n",
    "\n",
    "#   def __init__(self, train_df, val_df):\n",
    "#     self.label_dict = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "\n",
    "#     self.train_df = train_df\n",
    "#     self.val_df = val_df\n",
    "\n",
    "#     self.base_path = '/multinli_1.0/'\n",
    "#     self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n",
    "#     self.train_data = None\n",
    "#     self.val_data = None\n",
    "#     self.init_data()\n",
    "\n",
    "#   def init_data(self):\n",
    "#     self.train_data = self.load_data(self.train_df)\n",
    "#     self.val_data = self.load_data(self.val_df)\n",
    "\n",
    "#   def load_data(self, df):\n",
    "#     MAX_LEN = 512\n",
    "#     token_ids = []\n",
    "#     mask_ids = []\n",
    "#     seg_ids = []\n",
    "#     y = []\n",
    "\n",
    "#     premise_list = df['sentence1'].to_list()\n",
    "#     hypothesis_list = df['sentence2'].to_list()\n",
    "#     label_list = df['gold_label'].to_list()\n",
    "\n",
    "#     for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\n",
    "#       premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
    "#       hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n",
    "#       pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "#       premise_len = len(premise_id)\n",
    "#       hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "#       segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "#       attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "#       token_ids.append(torch.tensor(pair_token_ids))\n",
    "#       seg_ids.append(segment_ids)\n",
    "#       mask_ids.append(attention_mask_ids)\n",
    "#       y.append(self.label_dict[label])\n",
    "    \n",
    "#     token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "#     mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "#     seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "#     y = torch.tensor(y)\n",
    "#     dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "#     print(len(dataset))\n",
    "#     return dataset\n",
    "\n",
    "#   def get_data_loaders(self, batch_size=32, shuffle=True):\n",
    "#     train_loader = DataLoader(\n",
    "#       self.train_data,\n",
    "#       shuffle=shuffle,\n",
    "#       batch_size=batch_size\n",
    "#     )\n",
    "\n",
    "#     val_loader = DataLoader(\n",
    "#       self.val_data,\n",
    "#       shuffle=shuffle,\n",
    "#       batch_size=batch_size\n",
    "#     )\n",
    "\n",
    "#     return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3d6004a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392702\n",
      "9815\n"
     ]
    }
   ],
   "source": [
    "# mnli_dataset = MNLIDataBert(multinli_train_reduced, multinli_test_reduced) #for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8e2baf27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mnli_dataset.train_data[2][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1063e2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 101, 2129, 2079, 2017, 2113, 1029, 2035, 2023, 2003, 2037, 2592, 2153,\n",
       "         1012,  102, 2023, 2592, 7460, 2000, 2068, 1012,  102,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mnli_dataset.train_data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e58e359",
   "metadata": {},
   "source": [
    "# Preparing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0118abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "encoding_dict= {\"gold_label\": {\"entailment\":0, \"contradiction\":1,\"neutral\":2}}\n",
    "multinli_train_reduced2 = multinli_train_reduced.replace(encoding_dict)\n",
    "multinli_test_reduced2 = multinli_test_reduced.replace(encoding_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bf9d5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "multinli_train_reduced2, musltinli_val = train_test_split(multinli_train_reduced2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "896a012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your train examples. You need more than just two examples...\n",
    "sen1 = list(multinli_train_reduced2.sentence1)\n",
    "sen2 = list(multinli_train_reduced2.sentence1)\n",
    "\n",
    "resulting_list = []\n",
    "for a,b,label in zip(sen1,sen2,list(multinli_train_reduced2.gold_label)):\n",
    "    resulting_list.append(InputExample(texts=[a, b], label=label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "40693f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for val set\n",
    "#Define your train examples. You need more than just two examples...\n",
    "sen1 = list(musltinli_val.sentence1)\n",
    "sen2 = list(musltinli_val.sentence1)\n",
    "\n",
    "resulting_list_val = []\n",
    "for a,b,label in zip(sen1,sen2,list(musltinli_val.gold_label)):\n",
    "    resulting_list_val.append(InputExample(texts=[a, b], label=label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec9a01e",
   "metadata": {},
   "source": [
    "# Training (Bert+maxpooling+Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6a3d6ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Iteration:   0%|          | 7/39271 [01:06<103:46:22,  9.51s/it]\n",
      "Epoch:   0%|          | 0/1 [01:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mb/1vwyh_q926n5d19zx_zrlkw80000gn/T/ipykernel_1151/903771269.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#Tune the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_objectives\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_evaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    704\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                         \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    \n",
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(resulting_list, shuffle=True, batch_size=10)\n",
    "train_loss = losses.SoftmaxLoss(model,num_labels=3,sentence_embedding_dimension=word_embedding_model.get_word_embedding_dimension())\n",
    "test_evaluator= evaluation.EmbeddingSimilarityEvaluator.from_input_examples(resulting_list_val, batch_size=10)\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],evaluator=test_evaluator, epochs=1, warmup_steps=100, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c8d37",
   "metadata": {},
   "source": [
    "# Testing on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bbc7784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your test examples\n",
    "sen1 = list(multinli_test_reduced2.sentence1)\n",
    "sen2 = list(multinli_test_reduced2.sentence1)\n",
    "\n",
    "resulting_list_test = []\n",
    "for a,b,label in zip(sen1,sen2,list(multinli_test_reduced2.gold_label)):\n",
    "    resulting_list_test.append(InputExample(texts=[a, b], label=label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5345735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, evaluation\n",
    "\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(resulting_list_test)\n",
    "\n",
    "evaluator(model, output_path = \"./eval_testset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca47b7",
   "metadata": {},
   "source": [
    "# END IS HERE <<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32d2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#some code is from the docs, which you might want to find here --> \n",
    "#https://www.sbert.net/docs/training/overview.html \n",
    "\n",
    "#Define the model. We do the basic bert + mean Pooling\n",
    "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7813af6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-1.76214516e-01  1.20601311e-01 -2.93623805e-01 -2.29858026e-01\n",
      " -8.22923705e-02  2.37709388e-01  3.39984953e-01 -7.80964315e-01\n",
      "  1.18127719e-01  1.63373917e-01 -1.37715399e-01  2.40282431e-01\n",
      "  4.25125599e-01  1.72417775e-01  1.05279572e-01  5.18164217e-01\n",
      "  6.22221269e-02  3.99285942e-01 -1.81652486e-01 -5.85578620e-01\n",
      "  4.49718162e-02 -1.72750562e-01 -2.68443406e-01 -1.47386089e-01\n",
      " -1.89217791e-01  1.92150444e-01 -3.83842468e-01 -3.96006793e-01\n",
      "  4.30648834e-01 -3.15319479e-01  3.65949929e-01  6.05155788e-02\n",
      "  3.57325852e-01  1.59736261e-01 -3.00983965e-01  2.63250142e-01\n",
      " -3.94310951e-01  1.84855536e-01 -3.99549067e-01 -2.67889649e-01\n",
      " -5.45117319e-01 -3.13405395e-02 -4.30643976e-01  1.33278191e-01\n",
      " -1.74793944e-01 -4.35465515e-01 -4.77378905e-01  7.12556392e-02\n",
      " -7.37002045e-02  5.69137037e-01 -2.82579571e-01  5.24973609e-02\n",
      " -8.20008039e-01  1.98296845e-01  1.69511810e-01  2.71779805e-01\n",
      "  2.64610738e-01 -2.55738590e-02 -1.74096450e-01  1.63314268e-01\n",
      " -3.95260781e-01 -3.17559876e-02 -2.62555957e-01  3.52754444e-01\n",
      "  3.01434726e-01 -1.47197261e-01  2.10075781e-01 -1.84010491e-01\n",
      " -4.12895858e-01  4.14775789e-01 -1.89769417e-01 -1.35482132e-01\n",
      " -3.79272252e-01 -4.68026213e-02 -3.33601311e-02  9.00393873e-02\n",
      " -3.30132782e-01 -3.87314595e-02  3.75082165e-01 -1.46996781e-01\n",
      "  4.34959680e-01  5.38325608e-01 -2.65445173e-01  1.64445847e-01\n",
      "  4.17078346e-01 -4.72507104e-02 -7.48732761e-02 -4.26260889e-01\n",
      " -1.96994528e-01  6.10317141e-02 -4.74263072e-01 -6.48334861e-01\n",
      "  3.71462375e-01  2.50956714e-01  1.22529857e-01  8.88766050e-02\n",
      " -1.06724493e-01  5.33983707e-02  9.74506959e-02 -3.46654728e-02\n",
      " -1.02882974e-01  2.32288897e-01 -2.53739655e-01 -5.13112187e-01\n",
      "  1.85216293e-01 -3.04357767e-01 -3.55209485e-02 -1.26974970e-01\n",
      " -7.71633461e-02 -5.15329897e-01 -2.28071839e-01  2.03346424e-02\n",
      "  7.38176107e-02 -1.52558237e-01 -4.00837779e-01 -2.47749388e-01\n",
      "  3.97470444e-01 -2.60260552e-01  2.50905782e-01  1.68228969e-01\n",
      "  1.33900151e-01 -2.10833736e-02 -4.70035255e-01  4.78849858e-01\n",
      "  2.80345649e-01 -4.64546949e-01  3.21746647e-01  2.34207124e-01\n",
      "  2.45772064e-01 -4.71482247e-01  5.00401378e-01  4.10190225e-01\n",
      "  5.15216887e-01  2.62549400e-01  2.11590920e-02 -3.89687479e-01\n",
      " -2.41743028e-01 -2.14834601e-01 -8.62649977e-02 -1.65323347e-01\n",
      " -5.21896258e-02  3.41875136e-01  4.50314581e-01 -3.06973577e-01\n",
      " -2.02294275e-01  6.85521662e-01 -5.33892334e-01  3.58471334e-01\n",
      "  1.45286307e-01 -7.07059056e-02 -1.50529519e-01 -8.56280848e-02\n",
      " -7.67850801e-02  1.89544752e-01 -1.04067117e-01  5.33543825e-01\n",
      " -5.27887285e-01  2.42331661e-02 -2.64347792e-01 -2.23186597e-01\n",
      " -3.81208837e-01  7.59914666e-02 -4.64485049e-01 -3.36549282e-01\n",
      "  4.21229661e-01  1.07479110e-01  1.90457791e-01  2.89499294e-03\n",
      " -1.08513519e-01  1.53545424e-01  3.16023320e-01 -2.70839240e-02\n",
      " -5.40594518e-01  8.97287801e-02 -1.15549415e-01  3.97803962e-01\n",
      " -4.97683257e-01 -2.84893423e-01  4.99865189e-02  3.61279428e-01\n",
      "  6.90535605e-01  1.46821290e-01  1.73396260e-01 -1.74582347e-01\n",
      " -3.15702498e-01  6.72997311e-02  2.17250168e-01  9.78537723e-02\n",
      " -1.29472762e-01 -1.86929911e-01  1.34878010e-01 -1.53885335e-01\n",
      "  7.44717270e-02 -1.85535997e-01 -2.80628145e-01 -1.14144042e-01\n",
      "  4.12249416e-01  6.39495850e-02 -1.45715296e-01 -9.82060805e-02\n",
      " -1.33081838e-01 -1.88410431e-01 -2.84842215e-02 -3.49508896e-02\n",
      "  3.34260538e-02  6.98896274e-02  1.90354243e-01 -2.96723843e-01\n",
      "  2.64712470e-03  1.09140888e-01  1.70894749e-02  2.60589242e-01\n",
      "  3.29038084e-01 -6.61558360e-02  2.39665508e-01 -2.26194829e-01\n",
      " -3.36867012e-02  1.49400383e-01 -3.21265340e-01 -2.68577784e-01\n",
      "  5.72631717e-01 -4.92308319e-01  2.00666457e-01 -3.49261582e-01\n",
      " -2.89887227e-02  6.09010458e-01 -5.72333217e-01  2.35000357e-01\n",
      "  6.47170283e-03 -3.14949788e-02  2.78107245e-02 -3.90340835e-01\n",
      " -2.08949819e-01 -3.04452688e-01 -7.20196515e-02 -8.29839334e-02\n",
      "  3.73792946e-01  7.38937333e-02 -2.21076012e-02  9.88139734e-02\n",
      " -1.51426390e-01 -1.40430897e-01  2.26017982e-01  2.76090145e-01\n",
      " -8.87749568e-02 -1.12815984e-01 -2.66286075e-01  2.77834415e-01\n",
      " -4.75611277e-02  6.71007335e-02 -2.78586969e-02 -2.39991322e-02\n",
      "  2.51708657e-01  4.68793720e-01 -5.39325595e-01  1.10598676e-01\n",
      " -3.44947338e-01  4.15989727e-01  7.28481635e-02 -3.19647491e-01\n",
      "  4.90374207e-01 -7.30351312e-03 -2.64239078e-03  9.63711083e-01\n",
      "  3.23885024e-01 -7.79620111e-02 -2.37589359e-01  2.34038755e-01\n",
      " -3.16054136e-01 -1.65651855e-03 -1.09070659e+00  3.38409126e-01\n",
      "  4.70604971e-02  1.07435554e-01 -2.06672251e-01  4.26455634e-03\n",
      " -1.38482219e-03 -5.31455457e-01 -2.75648445e-01 -1.64648458e-01\n",
      " -3.42916429e-01 -4.26118791e-01  6.01812303e-01  4.55971777e-01\n",
      " -2.72702157e-01 -3.45806405e-02  2.62752473e-01 -6.34195236e-03\n",
      "  2.79630989e-01 -2.53558844e-01 -1.68626383e-01  3.82936336e-02\n",
      "  2.07763016e-01 -4.31525826e-01 -7.23999217e-02 -1.26854599e-01\n",
      "  2.07030401e-02  5.74441195e-01  3.54672313e-01  9.28301290e-02\n",
      "  6.70509264e-02  1.11520492e-01 -1.86511576e-02  4.62351829e-01\n",
      "  2.72504598e-01 -3.60474080e-01  5.29415250e-01 -1.00336620e-03\n",
      " -8.81360844e-02  1.49975479e-01  5.25861345e-02  4.63517725e-01\n",
      " -3.96831453e-01  2.42640555e-01 -2.08912224e-01  3.65671992e-01\n",
      " -4.73421358e-04  5.33963144e-01 -1.97879657e-01  3.11583161e-01\n",
      " -6.96714759e-01 -4.29500520e-01 -4.49359298e-01 -2.71368846e-02\n",
      " -6.98711723e-02  2.06174776e-01 -1.57107815e-01  4.43521351e-01\n",
      " -6.74266815e-02 -3.00924182e-01  5.14859498e-01  3.36029112e-01\n",
      "  6.63376078e-02 -1.15234904e-01 -2.95979511e-02  2.79471785e-01\n",
      " -3.48200314e-02 -7.29324371e-02 -4.58473936e-02  1.54262573e-01\n",
      "  8.09356034e-01  5.20327806e-01 -4.02114719e-01 -3.23150121e-02\n",
      " -1.10364065e-01  7.50503540e-02 -1.51098520e-01  8.45740080e-01\n",
      " -1.80843830e-01  3.22573453e-01  1.04708031e-01  3.19663852e-01\n",
      " -1.55085474e-01  1.69236720e-01 -2.56996661e-01  2.01208934e-01\n",
      "  1.77392960e-01 -2.74333149e-01 -3.36944133e-01  5.02356768e-01\n",
      " -1.18357174e-01 -2.01167211e-01 -5.36485612e-01 -7.69809037e-02\n",
      "  1.15380567e-02 -2.36464322e-01 -2.98769511e-02  1.31366819e-01\n",
      "  2.94184446e-01  9.90917310e-02 -5.43897450e-01  1.40812799e-01\n",
      "  3.66998821e-01  5.04863933e-02  1.99122533e-01 -2.80674487e-01\n",
      "  4.34192181e-01 -1.40275270e-01  5.78048766e-01  1.77715912e-01\n",
      "  8.98361728e-02  3.29651535e-01  6.13006689e-02 -3.24933469e-01]\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [ 0.32208765 -0.00123915  0.17937355 -0.36919153 -0.0646024   0.09153675\n",
      "  0.24119096 -0.29494187  0.07728934  0.11577022 -0.04479973  0.17928232\n",
      "  0.14753601  0.21511652  0.3681081   0.20910884  0.2719421   0.34880075\n",
      " -0.5725191  -0.18253207  0.44489565  0.2745295   0.04266283 -0.0768356\n",
      "  0.1868914   0.4496502  -0.16932607 -0.24896358 -0.2047927   0.4028505\n",
      " -0.21019278  0.03775724  0.0784851   0.12848447  0.02593074  0.4715595\n",
      "  0.17853808 -0.07379774  0.08130728 -0.23328763 -0.49801257 -0.04135676\n",
      " -0.12094615  0.17028989 -0.19154087 -0.38459828 -0.77479154 -0.10622745\n",
      " -0.230449    0.40241435 -0.8745085   0.23853713 -0.47129878  0.21262155\n",
      "  0.33409327 -0.24154009 -0.14835101 -0.14513582 -0.34830913 -0.08349229\n",
      " -0.6909729  -0.29845273 -0.12230499  0.07482649 -0.18775588 -0.37546536\n",
      "  0.21369524 -0.10096434 -0.12234413  0.31431514 -0.23989938  0.22460747\n",
      "  0.03996023  0.36034817 -0.5663804   0.21883497  0.11020307 -0.10870802\n",
      "  0.07084118 -0.02608197  0.18370357  0.08465934 -0.20478226 -0.2443561\n",
      " -0.08180566 -0.01903087 -0.03591361  0.02398438 -0.28558552  0.07374776\n",
      " -0.29744214 -0.8771784   0.47101927 -0.04940468  0.363945    0.4826438\n",
      "  0.01564611  0.03558946 -0.26203018 -0.11218462  0.02411043  0.37477797\n",
      " -0.09897301 -0.09851873  0.1500087   0.00689568 -0.12652434 -0.3159895\n",
      "  0.31449512 -0.2942561  -0.26941046  0.20221168  0.1432989  -0.19584623\n",
      " -0.34104455 -0.03172736  0.7365027   0.3192352   0.2438129   0.30732557\n",
      "  0.09933221  0.19010916 -0.10694544  0.05178654  0.03233407 -0.10314638\n",
      "  0.26499185  0.31206453  0.43152592 -0.64261234  0.08409572 -0.04327369\n",
      " -0.04991201 -0.12718552  0.1378921   0.01306234  0.34383243  0.09234285\n",
      " -0.09922758 -0.52159923  0.25842282 -0.01057153 -0.0047816   0.03938875\n",
      "  0.1908606   0.32933906 -0.2434513  -0.07328291 -0.3928002   0.1454179\n",
      "  0.32839537 -0.04184642  0.0740708  -0.73860544 -0.09076004  0.15802306\n",
      " -0.09780032 -0.21605963 -0.30027467  0.23236565  0.01072465  0.49570504\n",
      "  0.04974862  0.29931414 -0.05382223  0.35328108  0.34191778  0.4966726\n",
      " -0.4860526  -0.19098853  0.8154574   0.2296264  -0.3207779  -0.3272672\n",
      " -0.36771715  0.34521168 -0.02620149 -0.14315043  0.10648441 -0.24638028\n",
      " -0.09366625  0.17198654 -0.08508826  0.2012033  -0.05879208 -0.34020996\n",
      " -0.19565335  0.28280842  0.20124315 -0.08207261  0.09779137 -0.26374987\n",
      "  0.12176564 -0.01041447 -0.43859828  0.11058244  0.48010397 -0.10981973\n",
      " -0.6375458   0.2933677  -0.1920764   0.46537     0.27042025  0.19388466\n",
      "  0.17379065 -0.30077016 -0.02751149 -0.02291272  0.3678464   0.02492155\n",
      "  0.5370548   0.18851218 -0.13344434  0.08917331  0.05542922 -0.24818331\n",
      " -0.04199769  0.05767404 -0.18278803 -0.4168642   0.16070572 -0.4636253\n",
      "  0.11769223 -0.37706912  0.02960378  0.6925613  -0.4830891   0.21128388\n",
      "  0.18214521 -0.18429571  0.06817672 -0.02460921 -0.19073592 -0.06736959\n",
      " -0.5670072  -0.23929319 -0.08497225  0.03093973  0.31079924  0.12916228\n",
      "  0.05248247 -0.3344981   0.18810096  0.23547138 -0.00183461  0.45361587\n",
      "  0.24885091 -0.05641093 -0.29774585 -0.43511727 -0.07969442 -0.17670132\n",
      " -0.13347073  0.1938272   0.22002597 -0.1105753   0.26473752 -0.27179077\n",
      "  0.03410919 -0.47714397  0.44719064 -0.05570377  0.3964373   0.27483252\n",
      "  0.33305582 -0.10890224  0.27888152  0.21596918 -0.05252238 -0.35867527\n",
      " -0.6906288   0.03960172  0.006528   -0.01095338 -0.10027695  0.04770022\n",
      " -0.34146905 -0.16714147  0.0713641  -0.18078452 -0.30248472 -0.6842871\n",
      " -0.09592862 -0.2141109  -0.65524364  0.56756437  0.2694671  -0.00190075\n",
      "  0.8618065   0.16771564  0.03102793 -0.26773074 -0.07830277 -0.48510876\n",
      " -0.2673721  -0.33354253 -0.57382506  0.35678264  0.08993587 -0.13057192\n",
      " -0.15136498 -0.06124158 -0.13037069  0.5585604   0.614175   -0.04804063\n",
      " -0.06388566  0.08390598 -0.2514366  -0.04359836 -0.18525806  0.04693348\n",
      " -0.3438084  -0.09738493  0.16833657  0.07526833  0.17694484  0.17727178\n",
      " -0.03423444  0.14993559 -0.13773187 -0.20949689 -0.6127286   0.3781398\n",
      "  0.39018276 -0.08359334  0.03152153  0.1312238   0.38826072  0.21844244\n",
      "  0.09724277  0.42089376 -0.3264124  -0.26933405 -0.39095095 -0.22648686\n",
      " -0.32020715 -0.16287392 -0.03581632  0.36373863  0.18583268 -0.02914038\n",
      " -0.46577966  0.291689    0.37251285 -0.23726629  0.00338654  0.41541013\n",
      "  0.03300446  0.45003942 -0.08159252  0.33990324  0.24497877  0.02352422\n",
      " -0.14643069 -0.12644547  0.31128654 -0.15182625  0.01009407  0.4910855\n",
      "  0.14362389  0.11589053 -0.23236994  0.2475176   0.18364492 -0.24836832\n",
      " -0.11220928 -0.23113328  0.08428954 -0.24378629  0.1330728   0.42355707\n",
      "  0.33348343 -0.34370166  0.03443642  0.1879552   0.2003718  -0.05355949\n",
      "  0.28485262  0.07176548  0.05487137 -0.08103793  0.27076903  0.11700268]\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [ 0.5897935  -0.23598306 -0.25411728  0.003116   -0.08485726 -0.26799735\n",
      " -0.0750668  -0.3002136   0.05151681  0.16585353  0.26076773  0.38256386\n",
      "  0.43732896 -0.09301952 -0.26568773 -0.09716267 -0.48096034  0.11878297\n",
      "  0.13675484  0.04712062 -0.2369652  -0.5233236  -0.01631868  0.06127293\n",
      " -0.74333024 -0.11898937 -0.7886529  -0.48108873  0.10314932 -0.32372442\n",
      "  0.8144374  -0.39774546 -0.5031561  -0.797246   -0.63248247  0.32320955\n",
      " -0.3841939  -0.11186657 -0.13243596  0.02069727 -0.14309543 -0.03701181\n",
      "  0.06116629  0.1633292  -0.11174303  0.2523423  -1.0464073  -0.3725238\n",
      "  0.15601969 -0.29991567  0.19883858  0.23433448 -0.37025785  0.3173359\n",
      "  0.84428614  0.06977677  0.03273625  0.09948341 -0.31141323  0.50517696\n",
      "  0.00309278  0.38013712  0.04582768  0.00633351 -0.00142962 -0.13568671\n",
      " -0.07611399 -0.2584428  -0.8022127   0.5508589  -0.0912439  -0.21782024\n",
      " -0.788109   -0.5118382   0.4667251   0.55274767 -0.3712475  -0.18645339\n",
      "  0.3585699  -0.19586371  0.18042575 -0.4254886  -0.09681415 -0.05536788\n",
      "  0.524893    0.24481143  0.01934667 -0.29637957 -0.12777862 -0.30534953\n",
      "  0.4534936   0.07469123 -0.07061679  0.2624298   0.37383917  0.14306353\n",
      "  0.00127875 -0.4177606  -0.24014072 -0.25093535  0.3484378   0.31144068\n",
      "  0.08087319 -0.57640535  0.5408529  -0.01802227 -0.129598   -0.07399654\n",
      "  0.39369777  0.6488383  -0.02029987 -0.56655574  0.29676     0.5200026\n",
      "  0.2153876   0.10369668  0.06199232  0.01896303 -0.15269193 -1.064266\n",
      "  0.76149625  0.20734398  0.44718912  0.14493936  0.6580227  -0.09440911\n",
      " -0.23316365  0.42157066  0.11957637 -0.32571045  0.16425513 -0.49508664\n",
      " -0.19516093 -0.5618325  -0.14933242  0.6109411  -0.17897958 -0.01805528\n",
      " -0.59640485  0.04918614  0.15347832 -0.42829418  0.7329528  -0.352911\n",
      " -0.11159644  0.06127824 -0.2970443   0.43966582 -0.09660369  0.65579444\n",
      " -0.614034    0.0257661   0.43827462  0.01733233 -0.40002266 -0.0817837\n",
      " -0.3712697   0.08230258 -0.13104409 -0.53261083 -0.29928377  0.6993659\n",
      " -0.04398754 -0.15703018  0.09794132 -0.03017488 -0.10002714  0.19996567\n",
      " -0.4818853   0.17949128  0.56566006 -0.11954812 -0.69637316  0.05259673\n",
      " -0.00549638  0.16739352 -0.3169286  -0.09747565  0.3319366   0.47199607\n",
      "  0.12653996  0.19130988  0.42949042  0.5529124   0.31463322 -0.31433097\n",
      " -0.41508675  0.32897723  0.35702696 -0.19209623  0.22239399 -0.48717877\n",
      "  0.34091556 -0.22137465 -0.12667574  0.21120799 -0.31347883  0.84689397\n",
      "  0.2011265  -0.42598772  0.51315707 -1.2351416   0.7697179  -0.17414272\n",
      " -0.02181113 -0.03568644 -1.1059494  -0.5720654   0.05585185  0.12461492\n",
      " -0.45065868  0.06428991 -0.16033874  0.39932933 -0.10322893 -0.02025482\n",
      " -0.18010439  0.06234799 -0.02188893 -0.15795405  0.28316957  0.02385321\n",
      "  0.03098092 -0.07853284  0.2989655  -0.06237324  0.54986775  0.1786233\n",
      "  0.21164747  0.444834    0.04890749 -0.16238077 -0.2266991   0.18871988\n",
      "  0.07943629  0.13597561 -0.18484475  1.113551    0.8280956  -0.31202677\n",
      "  0.09505969  0.05096104  0.3880489   0.25000462  0.5584864   0.31088746\n",
      " -0.05318557 -0.07675337  0.15282334  0.09189966 -0.01429159  0.6657541\n",
      " -0.03346024 -0.44703522  0.80067486 -0.47992817  0.17478204 -0.3056387\n",
      "  0.55365235  0.42380932  0.4867432  -0.49677992 -0.45194814 -0.9556308\n",
      " -0.20709991 -0.22605748 -0.00999173  0.98797697  0.5880775   0.08305465\n",
      " -0.55781347  0.2113681  -0.36072204  0.5266853   0.33983573 -0.15756187\n",
      "  0.00423787 -0.05354505 -0.5777672   0.55951065 -0.05747151  0.16837639\n",
      "  0.37946835 -0.2577642   0.08421478 -0.1522991  -0.03280811  0.10083846\n",
      " -0.41858307 -0.4449903  -0.29309928  0.61442095  0.08548153 -0.06349552\n",
      " -0.61525536  0.795441   -0.24058378  0.20638883 -0.51252574  0.63120097\n",
      "  0.3674433  -0.4400991   0.46913972  0.23087747 -0.13737981  0.2169689\n",
      "  0.4004325  -0.02490645 -1.1396756   0.026539   -0.32730198  0.09984162\n",
      "  0.05725666 -0.84722185  0.06451982  0.45698047  0.63562983  0.45185626\n",
      " -0.27519056  0.21346176  0.17374258  0.42822036 -0.65845364  0.4000257\n",
      " -0.02035553 -0.6730787  -1.0269235   0.1687724  -0.09248722 -0.79977626\n",
      "  0.3809339   0.51712304  0.04200912 -0.04867568 -0.1877227   0.16339512\n",
      " -0.2197492   0.21939301  0.03676535 -0.29750276 -0.3740965  -0.52095073\n",
      " -0.41314635 -0.48947716 -0.81896615  0.08531491  0.34576964  0.12505995\n",
      "  0.24945211 -0.25254712 -0.03156152  0.2757313  -0.60857195  0.33570004\n",
      "  0.22913174  0.6607084  -0.302158   -0.05315318  0.22247505  0.06138734\n",
      "  0.3355515  -0.08485156  0.08764581  0.10872034 -0.40389308 -0.14949764\n",
      "  0.1945849  -0.81060654  0.7973097  -0.4116255   0.01364147  0.23472948\n",
      " -0.09732261 -0.29044065  0.03843189 -0.07090431 -0.17404495 -0.44859383\n",
      " -0.31867242  0.41656065 -0.05431653  0.14036177  1.0559161   0.53018165]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #that's actually the sbert one\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# #Our sentences we like to encode\n",
    "# sentences = ['This framework generates embeddings for each input sentence',\n",
    "#     'Sentences are passed as a list of string.',\n",
    "#     'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "# #Sentences are encoded by calling model.encode()\n",
    "# embeddings = model.encode(sentences)\n",
    "\n",
    "# #Print the embeddings\n",
    "# for sentence, embedding in zip(sentences, embeddings):\n",
    "#     print(\"Sentence:\", sentence)\n",
    "#     print(\"Embedding:\", embedding)\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da8c01c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d4444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_train=pd.read_csv(\"stsbenchmark/sts-train.csv\",sep='\\t',header=None,usecols=[4, 5, 6], quoting=QUOTE_NONE,names=[\"label\",\"sen1\",\"sen2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63db73a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5749.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.464398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "count  5749.000000\n",
       "mean      2.700999\n",
       "std       1.464398\n",
       "min       0.000000\n",
       "25%       1.500000\n",
       "50%       3.000000\n",
       "75%       3.800000\n",
       "max       5.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unlike describe in the assignment the similarity is [0,5], not [1,5]\n",
    "sts_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8e6efe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# # model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
    "# pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "# model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf831d49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>\n"
     ]
    }
   ],
   "source": [
    "# text = \"This fucking thing better works.\"\n",
    "\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# print(type(encoded_input))\n",
    "# output = model(**encoded_input)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = [\"Dummy example, does this shit even work?\",\"oimfowemfo\",\"Replace me by any text you'd like.\", \"WTF is this\"]#\"Dummy example, does this shit even work?\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt', padding=True)\n",
    "output = model(**encoded_input)\n",
    "output\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be044b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ok\n",
      "starting tokenizing the data\n"
     ]
    }
   ],
   "source": [
    "# #This here should hopefully make meaningful embeddings from BERT(not SBERT)\n",
    "\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# # model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model1 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# model2 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "# #load datasets\n",
    "# sts_train=pd.read_csv(\"stsbenchmark/sts-train.csv\",sep='\\t',header=None,usecols=[4, 5, 6], quoting=QUOTE_NONE,names=[\"label\",\"sen1\",\"sen2\"])\n",
    "# sts_test=pd.read_csv(\"stsbenchmark/sts-test.csv\",sep='\\t',header=None,usecols=[4, 5, 6], quoting=QUOTE_NONE,names=[\"label\",\"sen1\",\"sen2\"])\n",
    "# sts_dev=pd.read_csv(\"stsbenchmark/sts-dev.csv\",sep='\\t',header=None,usecols=[4, 5, 6], quoting=QUOTE_NONE,names=[\"label\",\"sen1\",\"sen2\"])\n",
    "# print(\"loading data ok\")\n",
    "# print(\"starting tokenizing the data\")\n",
    "\n",
    "\n",
    "# encoded_input1 = tokenizer(list(sts_train.sen1), return_tensors='pt', padding=True)\n",
    "# output = model1(**encoded_input1)\n",
    "# encoded_input2 = tokenizer(list(sts_train.sen2), return_tensors='pt', padding=True)\n",
    "# output2 = model1(**encoded_input2)\n",
    "\n",
    "\n",
    "# pooling_layer1= models.Pooling(200,pooling_mode_mean_tokens=True)\n",
    "# pooling_layer2= models.Pooling(200,pooling_mode_mean_tokens=True)\n",
    "\n",
    "\n",
    "# final_model1= SentenceTransformer(modules=[model1,pooling_layer1])\n",
    "# embedding1=final_model1.encode(encoded_input1, batch_size=128, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "\n",
    "# final_model2= SentenceTransformer(modules=[model2,pooling_layer2])\n",
    "# embedding2=final_model2.encode(encoded_input2, batch_size=128, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "\n",
    "# text = ['This fucking thing better works',\"Hate this bla bla, what the fuck is this for\"]\n",
    "# encoded_input = tokenizer(text, return_tensors='pt', padding=True)\n",
    "# output = final_model1(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "31aca2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sen1</th>\n",
       "      <th>sen2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.00</td>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.60</td>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.25</td>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           sen1  \\\n",
       "0   5.00                         A plane is taking off.   \n",
       "1   3.80                A man is playing a large flute.   \n",
       "2   3.80  A man is spreading shreded cheese on a pizza.   \n",
       "3   2.60                   Three men are playing chess.   \n",
       "4   4.25                    A man is playing the cello.   \n",
       "\n",
       "                                                sen2  \n",
       "0                        An air plane is taking off.  \n",
       "1                          A man is playing a flute.  \n",
       "2  A man is spreading shredded cheese on an uncoo...  \n",
       "3                         Two men are playing chess.  \n",
       "4                 A man seated is playing the cello.  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_sen1_encoding= tokenizer(list(sts_train[\"sen1\"]), padding=\"max_length\", truncation=True)\n",
    "# train_sen2_encoding= tokenizer(list(sts_train[\"sen2\"]), padding=\"max_length\", truncation=True)\n",
    "# sts_train.head(5) # we split the process up here so you can see the difference on the label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
